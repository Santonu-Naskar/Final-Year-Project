from google.colab import drive
!pip install transformers==4.28.0
!pip install datasets
!pip install scikit-learn

drive.mount('/content/drive')

from datasets import load_dataset, load_metric
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer
from google.colab import drive
from sklearn.metrics import f1_score

model = AutoModelForQuestionAnswering.from_pretrained("SpanBERT/spanbert-base-cased")
tokenizer = AutoTokenizer.from_pretrained("SpanBERT/spanbert-base-cased")

# Load dataset

squad = load_dataset("squad_v2")

def preprocess(example):
    question, context = example["question"], example["context"]
    encoding = tokenizer(question, context, truncation="only_second", padding="max_length", max_length=384)
    start_positions = []
    end_positions = []
    for x in example["answers"]:
      if(x["answer_start"]!=[ ]):
        start_positions.append(x["answer_start"][0])
        end_positions.append(x["answer_start"][0] + len(x["text"][0]))
      else:
        start_positions.append(0)
        end_positions.append(0)
    encoding.update({
        "start_positions": start_positions,
        "end_positions": end_positions,
    })
    return encoding

# Preprocess data
squad = squad.map(preprocess, batched=True, remove_columns=["question", "context", "id", "answers"])
squad.set_format("torch", columns=["input_ids", "attention_mask", "token_type_ids", "start_positions", "end_positions"])
print(squad)

args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=8,
    weight_decay=0.01,
    push_to_hub=False,
    logging_dir='./logs',
)

trainer = Trainer(model=model, args=args, train_dataset=squad["train"], eval_dataset=squad["validation"],)
trainer.train()

# Save the model and tokenizer
model.save_pretrained("saved_model/")
tokenizer.save_pretrained("saved_model/")
!zip -r /content/drive/MyDrive/finalyearproject/file.zip /content/saved_model
