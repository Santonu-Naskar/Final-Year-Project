from datasets import load_dataset
from sklearn.metrics import f1_score
from transformers import pipeline
from sklearn.metrics import confusion_matrix
tp = 0
tn=0
fp = 0
fn = 0
dataset = load_dataset("squad")
qa_pipeline = pipeline("question-answering", model="/content/drive/MyDrive/finalyearproject/saved_model", tokenizer="/content/drive/MyDrive/finalyearproject/saved_model")

predictions = []
for example in dataset["validation"]:
    context = example["context"]
    question = example["question"]
    prediction = qa_pipeline(context=context, question=question)
    if len(example["answers"]["text"])>0:
        if prediction["answer"] in example["answers"]["text"]:
            tp += 1
        else:
            fp += 1
    else:
        if prediction["answer"] == example["answers"]["text"]:
            tn += 1
        else:
            fn += 1
    predictions.append(prediction)

precision=tp/(tp+fp)
recall=tp/(tp+fn)
f1_scores=2*((precision*recall)/(precision+recall))

print("f1 score= ",f1_scores*100)
print("Accuracy= ",((tp+tn)/len(dataset["validation"]))*100)
